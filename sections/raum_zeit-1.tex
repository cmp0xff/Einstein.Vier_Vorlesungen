%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Raum und Zeit in der vorrelativistischen Physik}
\label{sec:rau-1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Die Relativitätstheorie ist aufs engste verbunden mit der Theorie von Raum und 
Zeit. Deshalb soll mit einer kurzen Untersuchung des Ursprungs unserer Ideen von 
Raum und Zeit begonnen werden, obwohl ich weiß, daß ich mich dabei auf 
strittiges Gebiet begebe. Alle Wissenschaft, sei es Naturwissenschaft oder 
Psychologie, sucht in gewisser Weise unsere Erlebnisse zu ordnen und in ein 
logisches System zu bringen. Wie hängen die geläufigen Ideen über Raum und Zeit 
mit dem Charakter unserer Erlebnisse zusammen?

Die Erlebnisse eines Menschen erscheinen uns als in eine Erlebnisreihe 
eingeordnet, in welcher die einzelnen unserer Erinnerung zugänglichen 
Einzelerlebnisse nach dem nicht weiter zu analysierenden Kriterium des 
\enquote{Früher} und \enquote{Später} geordnet erscheinen. Es besteht also für 
das Individuum eine Ich-Zeit oder subjektive Zeit\index{Zeit!subjektive}. Diese 
ist an sich nichts Meßbares. Ich kann zwar den Erlebnissen Zahlen zuordnen, 
derart, daß dem späteren Erlebnis eine größere Zahl zugeordnet wird als dem 
früheren, aber die Art dieser Zuordnung bleibt zunächst in hohem Maße 
willkürlich. Ich kann jedoch die Art dieser Zuordnung weiter fixieren durch eine 
Uhr, indem ich den durch sie vermittelten Erlebnisablauf mit dem Ablauf der 
übrigen Erlebnisse vergleiche. Unter einer Uhr versteht man ein Ding, welches 
abzählbare Erlebnisse liefert und noch andere Eigenschaften besitzt, von denen 
im folgenden die Rede sein wird.

Verschiedene Menschen können mit Hilfe der Sprache ihre Erlebnisse bis zu einem 
gewissen Grade miteinander vergleichen. Dabei zeigt sich, daß gewisse sinnliche 
Erlebnisse verschiedener Menschen einander entsprechen, während bei anderen ein 
solches Entsprechen nicht festgestellt werden kann. Jenen sinnlichen Erlebnissen 
verschiedener Individuen, welche einander entsprechen und demnach in gewissem 
Sinne überpersönlich sind, wird eine Realität gedanklich zugeordnet. Von ihr, 
daher mittelbar von der Gesamtheit jener Erlebnisse, handeln die 
Naturwissenschaften, speziell auch deren elementarste, die Physik. Relativ 
konstanten Erlebniskomplexen solcher Art entspricht der Begriff des 
physikalischen Körpers, speziell auch des festen Körpers. Die Uhr ist auch ein 
Körper bzw.\ ein körperliches System in diesem Sinne. Zum Wesen der Uhr gehört 
außerdem, daß die an ihr gezählten gleichartigen Teilvorgänge der Erlebnisfolge 
als einander gleich angesehen werden dürfen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Begriffe und Begriffssysteme erhalten die Berechtigung nur dadurch, daß sie zum 
Überschauen von Erlebniskomplexen dienen; eine andere Legitimation gibt es für 
sie nicht. Es ist deshalb nach meiner Überzeugung einer der verderblichsten 
Taten der Philosophen, daß sie gewisse begriffliche Grundlagen der 
Naturwissenschaft aus dem der Kontrolle zugänglichen Gebiete des 
Empirisch-Zweckmäßigen in die unangreifbare Höhe des Denknotwendigen 
(Apriorischen) versetzt haben. Denn wenn es auch ausgemacht ist, daß die 
Begriffe nicht aus den Erlebnissen durch Logik (oder sonstwie) abgeleitet werden 
können, sondern in gewissem Sinn freie Schöpfungen des menschlichen Geistes 
sind, so sind sie doch ebensowenig unabhängig von der Art der Erlebnisse, wie 
etwa die Kleider von der Gestalt der menschlichen Leiber. Dies gilt im 
besonderen auch von unseren Begriffen über Zeit und Raum, welche die Physiker -- 
von Tatsachen gezwungen -- aus dem Olymp des Apriori herunterholen mußten, um 
sie reparieren und wieder in einen brauchbaren Zustand setzen zu können.

Wir kommen nun zu den räumlichen Begriffen und Urteilen. Auch hier ist es 
unerläßlich, die Beziehung der Erlebnisse zu den Begriffen streng ins Auge zu 
fassen. Auf diesem Gebiete scheint mir 
\textsc{Poincar\'e}\index{\textsc{Poincar\'e}, H.} die Wahrheit besonders klar 
erfaßt zu haben in der Darstellung, welche er in seinem Buche: \enquote{La 
science et l'hypothèse} gegeben hat. Unter allen Veränderungen, welche wir an 
festen Körpern wahrnehmen, sind diejenigen durch Einfachheit ausgezeichnet, 
welche durch willkürliche Bewegungen unseres Körpers rückgängig gemacht werden 
können; \textsc{Poincar\'e}\index{\textsc{Poincar\'e}, H.} nennt sie 
\enquote{Änderungen der Lage}. Durch bloße Lagenänderungen kann man zwei Körper 
\enquote{aneinander anlegen}. Das Fundament der Geometrie (Kongruenzsätze) 
bezieht sich auf die Gesetze, welche jene Lagerungsmöglichkeiten beherrschen. 
Für den Raumbegriff scheint uns folgendes wesentlich. Man kann durch Anlegen von 
Körpern $B$, $C \ldots$ an einen Körper $A$ neu Körper bilden, wir wollen sagen, 
den Körper $A$ fortsetzen. Man kann einen Körper $A$ so fortsetzen, daß er mit 
jedem anderen Körper $X$ zur Berührung kommt. Wir können den Inbegriff aller 
Fortsetzungen des Körpers $A$ als den \enquote{Raum des Körpers $A$} bezeichnen. 
Dann gilt, daß alle Körper sich \enquote{im Raum des (beliebig gewählten) 
Körpers $A$} befinden. Man kann in diesem Sinne nicht von dem \enquote{Raum} 
schlechthin, sondern nur von dem \enquote{zu einem Körper $A$ gehörigen Raum} 
reden. Allerdings spielt im Alltagsleben der Körper Erdkruste eine so 
dominierende Rolle in der Beurteilung der Lagenverhältnisse der Körper, daß er 
zu dem ernstlich nicht zu verteidigenden Begriff \emph{des} Raumes (schlechthin) 
geführt hat. Wir wollen aber, um diesen verhängnisvollen Irrtum auszuschließen, 
nur von \enquote{Bezugskörper} oder \enquote{Bezugsraum} reden. Erst die 
allgemeine Relativitätstheorie hat eine Verfeinerung dieses Begriffes nötig 
gemacht, wie wir später sehen werden.

Ich will nicht näher auf diejenigen Eigenschaften des Bezugsraumes eingehen, 
welche dazu geführt haben, als Element des Raumes den Punkt einzuführen und den 
Raum als Kontinuum aufzufassen. Ebensowenig will ich zu analysieren versuchen, 
durch welche Eigenschaften des Bezugsraumes der Begriff der stetigen Punktreihe 
oder Linie gerechtfertigt sei. Sind aber diese Begriffe nebst ihrer Beziehung 
zum festen Körper der Erlebniswelt gegeben, so ist leicht zu sagen, was unter 
der Dreidimensionalität des Raumes zu verstehen ist, nämlich die Aussage: Jedem 
Punkt lassen sich drei Zahlen $x_1$, $x_2$ und $x_3$ (Koordinaten) zuordnen, 
derart, daß diese Zuordnung umkehrbar eindeutig ist, und daß sich $x_1$, $x_2$ 
und $x_3$ stetig ändern, wenn der zugehörige Punkt eine stetig Punktreihe 
(Linie) beschreibt.

Die vorrelativistische Physik setzt voraus, daß die Lagerungsgesetze idealer 
fester Körper der euklidischen Geometrie gemäß seien. Was dies bedeutet, kann 
z.\ B.\ wie folgt ausgedrückt werden. Zwei an einem festen Körper markierte 
Punkte bilden eine Strecke. Eine solche kann in mannigfacher Weise gegenüber dem 
Bezugsraume ruhend gelagert werden. Wenn nun die Punkte dieses Raumes so durch 
Koordinaten $x_1$, $x_2$, $x_3$ bezeichnet werden können, daß die 
Koordinatendifferenzen $\Dva x_1$, $\Dva x_2$, $\Dva x_3$ der 
Streckenpunkte bei jeder Lagerung der Strecke die nämliche Quadratsumme
\begin{align}
	s^2 = \Dva x_1^2 + \Dva x_2^2 + \Dva x_3^2
	\label{eq:1}
\end{align}
liefern, so nennt man den Bezugsraum \textsc{Euklid}isch und die Koordinaten 
kartesische\footnote{Diese Relation muß gelten für beliebige Wahl des 
Anfangspunktes und der Richtung (Verhältnis $\Dva x_1 : \Dva x_2 : \Dva 
x_3$) der Strecke.}. Es genügt hierfür sogar, diese Annahme in der Grenze für 
unendlich kleine Strecken zu machen. In dieser Annahme liegen einige weniger 
spezielle enthalten, auf die wir ihrer grundlegenden Bedeutung wegen aufmerksam 
machen wollen. Erstens nämlich wird vorausgesetzt, daß man einen idealen festen 
Körper beliebig bewegen könne. Zweitens wird vorausgesetzt, daß das 
Lagerungsverhalten idealer fester Körper in dem Sinne unabhängig vom Material 
des Körpers und von seinen Ortsänderungen ist, daß zwei Strecken, welche 
\emph{einmal} zur Deckung gebracht werden können, \emph{stets und überall} zur 
Deckung gebracht werden können. Diese beiden Voraussetzungen, welche für die 
Geometrie und überhaupt für die messende Physik von grundlegender Bedeutung 
sind, entstammen natürlich der Erfahrung; sie beanspruchen in der allgemeinen 
Relativitätstheorie allerdings nur für (gegenüber astronomischen Dimensionen) 
unendlich kleine Körper und Bezugsräume Gültigkeit.

Die Größe $s$ nennen wir die Länge der Strecke. Damit diese eindeutig bestimmt 
sei, muß die Länge einer bestimmten Strecke willkürlich festgesetzt, z.\ B.\ 
gleich $1$ gesetzt werden (Einheitsmaßstab). Dann sind die Längen aller übrigen 
Strecken bestimmt. Setzt man die $x_\nu$ linear abhängig von einem Parameter 
$\lambda$
\begin{align*}
	x_{\nu} = a_{\nu} + \lambda b_{\nu},
\end{align*}
so erhält man eine Linie, welche alle Eigenschaften der Geraden der euklidischen 
Geometrie besitzt. Speziell folgert man leicht, daß man durch $n$-maliges 
Abtragen einer Strecke $s$ auf einer Geraden eine Strecke von der Länge $n 
\cdot s$ erhält. Eine Länge bedeutet also das Ergebnis einer längs einer 
Geraden ausgeführten Messung mit Hilfe des Einheitsmaßstabes; sie hat ebenso 
wie die gerade Linie eine vom Koordinatensystem unabhängige Bedeutung, wie aus 
dem Folgenden hervorgeht.

Wir kommen nun zu einem Gedankengang, der in analoger Weise in der speziellen 
und allgemeinen Relativitätstheorie eine Rolle spielt. Wir fragen: Gibt es 
außer den verwendeten kartesischen Koordinaten noch andere gleichberechtigte? 
Die Strecke hat eine von der Koordinatenwahl unabhängige physikalische 
Bedeutung, ebenso also auch die Kugelfläche, welche man erhält als Ort der 
Endpunkte aller gleichen Strecken, welche man von einem beliebigen 
Anfangspunkt des Bezugsraumes aus abträgt. Sind sowohl $x_\nu$ als auch 
$x'_\nu$ ($\nu$ von $1$ bis $3$) kartesische Koordinaten unseres Bezugsraumes, 
so wird die Kugelfläche in bezug auf jene beiden Koordinatensysteme durch die 
Gleichungen ausgedrückt:
\begin{align}
	\sum \Dva x_{\nu}^{2} &= \text{konst.}
	\label{eq:2} \\
	\sum \Dva x^{\prime 2}_{\nu}&= \text{konst.}
	\label{eq:2a} \tag{2a}
\end{align}
Wie müssen sich die $x'_{\nu}$ aus den $x_{\nu}$ ausdrücken, damit die 
\cref{eq:2,eq:2a} äquivalent seien? Denkt man sich die $x'_{\nu}$ in Funktion 
der $x_{\nu}$ ausgedrückt, so kann man für genügend kleine $\Dva x_{\nu}$ 
nach dem \textsc{Taylor}schen Satze setzen: 
\begin{align*}
\Dva x'_{\nu} = 
	\sum_{\alpha} \frpa{x'_{\nu}}{x_{\alpha}}\,\Dva x_{\alpha} +
	\frac{1}{2} \sum_{\alpha\beta}
		\frpa{^2 x'_{\nu}}{x_{\alpha}\,\partial x_{\beta}}
			\,\Dva x_{\alpha}\,\Dva x_{\beta} \ldots
\end{align*}
Setzt man dies in \eqref{eq:2a} ein und vergleicht mit \eqref{eq:1}, so sieht
man, daß die $x'_{\nu}$ \emph{lineare} Gleichungen der $x_{\nu}$ sein müssen.
Setzt man demgemäß
\begin{align}
	x'_{\nu} &= a_{\nu} + \sum_\alpha b_{\nu\alpha} x_{\alpha}
	\label{eq:3}
\end{align}
oder
\begin{align}
	\Dva x'_{\nu} &= \sum_\alpha b_{\nu\alpha}\,\Dva x_{\alpha}
	\label{eq:3a} \tag{3a}
\end{align}
so drückt sich die Äquivalenz der \cref{eq:2,eq:2a} in der Form aus
\begin{align}
% Hier ist anders als das original Buch
\begin{aligned}
	\sum \Dva x^{\prime 2}_{\nu} = \lambda^2 \sum \Dva x_{\nu}^2
	\\
	\text{($\lambda$ von den unabhängig)}.
\end{aligned}
	\label{eq:2b} \tag{2b}
\end{align}
Hieraus folgt zunächst, daß $\lambda$ eine Konstante sein muß. Setzt man 
zunächst $\lambda = 1$, so liefern \eqref{eq:2b} und \eqref{eq:3a} die
Bedingungen
\begin{align}
	\sum_{\alpha} b_{\nu \alpha} b_{\nu\beta} = \delta_{\alpha\beta}\,,
	\label{eq:4}
\end{align}
wobei $\delta_{\alpha\beta} = 1$ oder $\delta_{\alpha\beta} = 0$ ist, je 
nachdem $\alpha = \beta$ oder $\alpha \neq \beta$. Die Bedingungen 
\eqref{eq:4} heißen Orthogonalitätsbedingungen, die Transformationen 
\eqref{eq:3}, $\eqref{eq:4}$ lineare orthogonale Transformationen. Verlangt 
man, daß $s^2 = \sum \Dva x_{\nu}^2$ für jedes Koordinatensystem gleich dem 
Quadrat der Länge sei und daß stets mit dem gleichen Einheitsmaßstabe gemessen 
werde, so muß $\lambda = 1$ sein. Dann sind die linearen orthogonalen 
Transformationen die einzigen, welche den Übergang von einem kartesischen 
Koordinatensystem eines Bezugsraumes zu einem anderen vermitteln. Man erkennt, 
daß bei Anwendung solcher Transformationen die Gleichungen einer Geraden 
wieder in die Gleichungen einer Geraden übergehen. Wir bilden noch die 
Umkehrung der Gleichungen \eqref{eq:3a}, indem wir beiderseits mit 
$b_{\nu\beta}$ multiplizieren und über $\nu$ summieren. Man erhält
\begin{align}
% Hier ist anders als das original Buch
\begin{aligned}
	\sum b_{\nu\beta}\, \Dva x'_{\nu} &= \sum_{\nu\alpha}
		b_{\nu\alpha} b_{\nu\beta} \,\Dva x_{\alpha}
		\\ 
		&= \sum_{\alpha}
		\delta_{\alpha\beta}\,\Dva x_{\alpha} = \Dva x_{\beta}\,.
\end{aligned}
	\label{eq:5}
\end{align}
Dieselben Koeffizienten $b$ vermitteln also auch die inverse Substitution der 
$\Dva x_{\nu}$. Geometrisch ist $b_{\nu\alpha}$ der Kosinus des Winkels 
zwischen der $x'_{\nu}$-Achse und der $x_{\alpha}$-Achse.

Zusammenfassend können wir sagen: In der euklidischen Geometrie gibt es (in 
einem gegebenen Bezugsraume) bevorzugte Koordinatensysteme, die kartesischen, 
welche auseinander durch lineare orthogonale Transformation der Koordinaten 
hervorgehen. In solchen Koordinaten drückt sich der mit dem Maßstab meßbare 
Abstand $s$ zweier Punkte des Bezugsraumes in besonders einfacher Weise aus. 
Auf diesen Begriff des Abstandes läßt sich die ganze Geometrie gründen. In der
gegebenen Darstellung bezieht sich die Geometrie auf wirkliche Dinge (feste 
Körper), und ihre Sätze sind Behauptungen über das Verhalten dieser Dinge, 
welche zutreffend oder auch unzutreffend sein können.

Gewöhnlich pflegt man die Geometrie so zu lehren, daß eine Beziehung der 
Begriffe zu den Erlebnissen nicht hergestellt wird. Es hat auch Vorteile, 
dasjenige, was an ihr rein logisch und von der prinzipiell unvollkommenen 
Empirie unabhängig ist, zu isolieren. Der reine Mathematiker kann sich damit 
begnügen. Er ist zufrieden, wenn seine Sätze richtig, d.\ h.\ ohne logische
Fehler aus den Axiomen abgeleitet sind. Die Frage, ob die euklidische 
Geometrie \emph{wahr} ist oder nicht, hat für ihn keinen Sinn. Für unseren 
Zweck aber ist es nötig, den Grundbegriffen der Geometrie Naturobjekte 
zuzuordnen; ohne eine solche Zuordnung ist die Geometrie für den Physiker 
gegenstandslos. Für den Physiker hat es daher wohl einen Sinn, nach der 
Wahrheit bzw.\ dem Zutreffen der geometrischen Sätze zu sprechen. Daß die so 
interpretierte euklidische Geometrie nicht nur Selbstverständliches, d.\ h.\ 
durch Definitionen logisch Bedingtes ausspricht, erkennt man durch folgende 
einfache Überlegung, welche von \textsc{Helmholtz} herrührt:

Zwischen $n$ Punkten des Raumes gibt es $\dfrac{1}{2} n(n-1)$ Abstände 
$s_{\mu\nu}$; zwischen diesen und den $3n$ Koordinaten bestehen die Relationen
\begin{align*}
s_{\mu\nu}^2 = \rbr{x_{1(\mu)} - x_{1(\nu)}}^2 +
	\rbr{x_{2(\mu)} - x_{2(\nu)}}^2 + \cdots
\end{align*}

Aus diesen $\dfrac{n(n-1)}{2}$ Gleichungen lassen sich die $3n$ Koordinaten
eliminieren, aus welcher Elimination mindestens $\dfrac{n(n-1)}{2} - 3n$ 
Gleichungen zwischen den $s_{\mu\nu}$ folgen müssen\footnote{In Wahrheit sind 
es $\dfrac{n(n-1)}{2} - 3n + 6$ Gleichungen.}. Da die $s_{\mu\nu}$ meßbare 
Größen sind, die ihrer Definition nach voneinander unabhängig sind, brauchen 
diese Beziehungen zwischen den $s_{\mu\nu}$ a priori nicht zu bestehen.

Aus dem Vorhergehenden zeigt sich, daß die Transformationsgleichungen 
\eqref{eq:3}, \eqref{eq:4} für die euklidische Geometrie eine fundamentale 
Bedeutung besitzen, indem sie den Übergang von einem kartesischen 
Koordinatensystem zu einem anderen beherrschen. Das kartesische 
Koordinatensystem zeichnet sich dadurch aus, daß sich in bezug auf jedes solche 
der meßbare Abstand $s$ zweier Punkte durch die Gleichung
\begin{align*}
s^2 = \sum \Dva x_{\nu}^2
\end{align*}
ausdrückt. Sind $K_{x_{\nu}}$ und $K'_{x'_{\nu}}$ zwei kartesische 
Koordinatensysteme, so gilt
\begin{align*}
\sum \Dva x_{\nu}^2 = \sum \Dva x^{\prime 2}_{\nu}\,.
\end{align*}

Die rechte Seite ist der linken identisch gleich vermöge der zwischen $x'$ und 
$x$ bestehenden linearen orthogonalen Transformationsgleichungen, und die 
rechte Seite unterscheidet sich von der linken nur dadurch, daß die $x_{\nu}$ 
durch die $x'_{\nu}$ ersetzt sind. Man drückt diesen Sachverhalt durch die 
Aussage aus: $\sum \Dva x_{\nu}^2$ ist eine Invariante bezüglich linearer 
orthogonaler Transformationen. Offenbar haben in der euklidischen Geometrie nur 
solche (und alle solche) Größen eine objektive (von der besonderen Wahl des 
kartesischen Systems unabhängige) Bedeutung, welche sich durch eine Invariante
(bezüglich linearer orthogonaler Koordinaten) ausdrücken lassen. Hierauf beruht 
es, daß die Invariantentheorie, welche sich mit den Strukturgesetzen der 
Invariante beschäftigt, für die analytische Geometrie von Bedeutung ist.

Als zweites Beispiel einer geometrischen Invariante nenne ich die Größe eines 
Volumens. Dasselbe drückt sich in der Form aus:
\begin{align*}
	V = \iiint \dd x_1\,\dd x_2\,\dd x_3\,.
\end{align*}
In der Tat ist nach dem \textsc{Jacobi}schen Transformationssatze
\begin{align*}
\iiint \dd x'_{1}\, \dd x'_{2}\, \dd x'_{3} = \iiint 
	\frpa{\rbr{x'_{1}, x'_{2}, x'_{3}}}{\rbr{x_{1}, x_{2}, x_{3}}}\,
	\dd x_1\,\dd x_2\,\dd x_3\,,
\end{align*}
wobei der Integrand im letzten Integral die Funktionaldeterminante der 
$x'_{\nu}$ nach den $x_{\nu}$ bedeutet, welche nach \eqref{eq:3} gleich der 
Determinante $\vbr{b_{\mu\nu}}$ der Substitutionskoeffizienten $b_{\nu\alpha}$ 
ist. Bildet man die Determinante der $\delta_{\mu\alpha}$ der \cref{eq:4}, so 
erhält man unter Anwendung des Multiplikationstheorems der Determinanten
\begin{align}
\begin{aligned}
1 &= \vbr{\delta_{\alpha\beta}} = \vbr{\sum_{\nu} b_{\nu\alpha} b_{\nu\beta}}
= \vbr{b_{\mu\nu}}^2; \\
\vbr{b_{\mu\nu}} &= \pm 1\, .
\end{aligned}
\label{eq:6}
\end{align}
Beschränkt man sich auf diejenigen Transformationen, welche die Determinante 
$+1$ haben\footnote{Es gibt also zweierlei kartesische Koordinatensysteme,
welche man als \enquote{Rechtssysteme} und \enquote{Linkssysteme} bezeichnet. 
Der Unterschied zwischen beiden ist jedem Physiker und Ingenieur geläufig. 
Interessant ist, daß man Rechtssysteme bzw.\ Linkssysteme an sich nicht 
geometrisch definieren kann, wohl aber die Gegensätzlichkeit beider Typen.} 
(und nur solche gehen aus \emph{stetiger} Änderung des Koordinatensystems 
hervor), so ist also $V$ eine Invariante.

Die Invariante ist aber nicht die einzige Form, welche gestattet, von der 
speziellen Wahl der kartesischen Koordinaten unabhängige Aussagen zum Ausdruck
zu bringen. Andere Ausdrucksmittel sind die Vektoren und Tensoren. Es handle 
sich z.\ B.\ um die Aussage, daß Punkte mit den (laufenden) Koordinaten 
$x_{\nu}$ auf einer Geraden liegen. Dann gilt
\begin{align*}
x_{\nu} - A_{\nu} = \lambda B_{\nu}\quad \text{($\nu$ von $1$ bis $3$)}\, .
\end{align*}
Ohne Beschränkung der Allgemeinheit kann hierbei
\begin{align*}
\Sigma B_{\nu}^2 = 1
\end{align*}
gesetzt werden.

Multipliziert man die Gleichungen mit $b_{\beta\nu}$ [vgl.\ \cref{eq:3a,eq:5}] 
und summiert über $\nu$, so erhält man
\begin{align*}
x'_{\beta} - A'_{\beta} = \lambda B'_{\beta}\,,
\end{align*}
wobei
\begin{align*}
B'_{\beta} = \sum_{\nu} b_{\beta\nu} B_{\nu};\qquad
A'_{\beta} = \sum_{\nu} b_{\beta\nu} A_{\nu}
\end{align*}
gesetzt ist. Dies sind die Gleichungen der Geraden bezüglich eines zweiten 
kartesischen Koordinatensystems $K'$. Sie haben dieselbe Form wie die 
Gleichungen bezüglich des ursprünglichen Koordinatensystems; es zeigt sich 
also, daß die Gerade eine vom Koordinatensystem unabhängige Bedeutung hat. 
Formal betrachtet beruht dies darauf, daß sich die Größen $\rbr{x_{\nu} - 
A_{\nu}} - \lambda B_{\nu}$ transformieren wie Streckenkomponenten 
$\Dva x_{\nu}$. Den Inbegriff dreier Größen, die für jedes kartesische 
Koordinatensystem definiert sind und sich transformieren wie 
Streckenkomponenten, nennt man einen Vektor. Verschwinden die drei 
Komponenten eines Vektors in bezug auf ein kartesisclles Koordinatensystem, so 
verschwinden sie auch für jedes andere, weil die Transformationsgleichungen 
homogen sind. So kann man die Bedeutung des Vektorbegriffes erfassen, ohne auf 
die geometrische Veranschaulichung rekurrieren zu müssen. Das geschilderte 
Verhalten der obigen Gleichung der Geraden drückt man so aus: Die Gleichung 
der Geraden ist bezüglich linearer orthogonaler Transformationen kovariant.

Nun soll kurz gezeigt werden, daß es geometrische Realitäten gibt, die auf den 
Begriff des Tensors führen. Es sei $P_0$ Mittelpunkt einer Fläche zweiten 
Grades, $P$ ein beliebiger Punkt der Oberfläche, $\xi_{\nu}$ seien die 
Projektionen der Strecke $P_0 - P$ auf die Koordinatenachsen. Dann ist
\begin{align*}
\sum_{\mu\nu} a_{\mu\nu} \xi_{\mu} \xi_{\nu}  = 1\, ,
\end{align*}
oder -- wie wir \emph{von nun an in allen} analogen Fällen unter Weglassung des 
Summenzeichens schreiben wollen, indem wir festsetzen, daß die Summation über 
zweimal auftretende Indizes selbstverständlich sei --
\begin{align*}
a_{\mu\nu} \xi_{\mu} \xi_{\nu}  = 1
\end{align*}
die Gleichung der Fläche. Die Größen $a_{\mu\nu}$ bestimmen die Fläche bis auf 
die Lage des Mittelpunktes in bezug auf das gewählte kartesische 
Koordinatensystem vollständig. Aus dem bekannten Transformationsgesetz der 
$\xi_{\nu}$ [\cref{eq:3a}] für lineare orthogonale Transformationen findet man 
leicht für die $a_{\mu\nu}$ das Transformationsgesetz\footnote{Die Gleichung 
$a'_{\sigma\tau} \xi'_{\sigma} \xi'_{\tau} = 1$ läßt sich vermöge \eqref{eq:5} 
durch $a'_{\sigma\tau} b_{\mu\sigma} b_{\nu\tau} \xi_{\sigma} \xi_{\tau} = 1$ 
ersetzen, woraus die Behauptung unmittelbar folgt.}
\begin{align*}
a'_{\sigma\tau} = b_{\sigma\mu} b_{\tau\nu} a_{\mu\nu}\,.
\end{align*}
Dies Transformationsgesetz ist homogen und vom ersten Grade in den 
$a_{\mu\nu}$. Die $a_{\mu\nu}$ nennt man vermöge dieses Transformationsgesetzes 
Komponenten eines Tensors vom zweiten Range\footnote{In der neueren Literatur 
wird der \enquote{Rang} eines Tensors häufig mit \enquote{Stufe} bezeichnet.} 
(letzteres wegen der Zwei-Zahl der Indizes). Verschwinden sämtliche Komponenten 
$a_{\mu\nu}$ eines Tensors in bezug auf ein kartesisches System, so 
verschwinden sie auch in bezug auf jedes andere kartesische System. Die Fläche 
zweiten Grades wird ihrer Form und Lage nach durch diesen Tensor $(a)$ 
dargestellt.

Es lassen sich Tensoren von beliebig hohem Range (Indexanzahl) analytisch 
definieren. Es erweist sich als möglich und zweckmäßig, Vektoren als Tensoren 
vom Range $1$, Invarianten (Skalare) als Tensoren vom Range $0$ anzusehen. Mit 
Rücksicht darauf läßt sich die Aufgabe der Invariantentheorie dahin 
formulieren: Nach welchen Gesetzen lassen sich aus gegebenen Tensoren neue 
bilden? Diese Gesetze wollen wir nun betrachten, um sie in der Folge anwenden zu 
können. Dabei handelt es sich zunächst nur um die Tensoren bezüglich linearer 
orthogonaler Transformationen, wie sie den Übergang von einem kartesischen 
System zu einem anderen desselben Bezugsraumes beherrschen. Da die Gesetze im 
,ganzen von der Dimensionszahl unabhängig sind, wollen wir letztere vorläufig 
unbestimmt lassen (Dimensionszahl $n$).

\emss{Definition.} Wenn ein Gebilde bezüglich 
jedes kartesischen Koordinatensystems eines Bezugsraumes von $n$ Dimensionen 
durch $n^{\alpha}$ Zahlen $A_{\mu\nu\varrho\ldots}$ ($\alpha = \text{Zahl der 
Indizes}$) definiert ist, so bilden diese die Komponenten eines Tensors vom 
Range $\alpha$, wenn ihr Transformationsgesetz
\begin{align}
A'_{\mu'\nu'\varrho' \ldots} = 
b_{\mu'\mu} b_{\nu'\nu} b_{\varrho'\varrho} \ldots A_{\mu\nu\varrho \ldots}
\label{eq:7}
\end{align}
ist.

Bemerkung: Aus dieser Definition folgt, daß
\begin{align}
A_{\mu\nu\varrho \ldots} B_{\mu} C_{\nu} D_{\varrho}
\label{eq:8}
\end{align}
eine Invariante ist, falls $(B)$, $(C)$, $(D) \ldots$ Vektoren sind. Umgekehrt 
kann der Tensorcharakter von $(A)$ gefolgert werden, wenn bekannt ist, daß die 
obige Bildung für beliebige Wahl der Vektoren $(B)$, $(C)$ usw.\ auf eine 
Invariante führt.

\emss{Addition und Subtraktion.} Durch Addition und Subtraktion 
entsprechender Komponenten von Tensoren gleichen Ranges entsteht wieder ein 
Tensor von gleichem Range:
\begin{align}
A_{\mu\nu\varrho\ldots} \pm B_{\mu\nu\varrho\ldots} = C_{\mu\nu\varrho\ldots}
\label{eq:9}
\end{align}

Beweis aus der obigen Definition des Tensors.

\emss{Multiplikation.} Aus einem Tensor vom Range $\alpha$ und einem Tensor vom 
Range \beta erhält man einen Tensor vom Range $\alpha + \beta$, indem man alle 
Komponenten des ersten mit allen Komponenten des zweiten multipliziert:
\begin{align}
T_{\mu\nu\varrho\ldots \alpha\beta\gamma\ldots} = A_{\mu\nu\varrho\ldots} 
B_{\alpha\beta\gamma\ldots}
\label{eq:10}
\end{align}

\emss{Verjüngung.} Aus einem Tensor vom Range $\alpha$ erhält man einen Tensor 
vom Range $\alpha - 2$, indem man zwei bestimmte Indizes einander gleich setzt 
und über diesen nunmehr einheitlichen Index summiert:
\begin{align}
T_{\varrho\ldots} = A_{\mu\mu\rho\ldots} 
	\rbr{= \sum_{\mu} A_{\mu\mu\rho\ldots}}.
\label{eq:11}
\end{align}

Beweis:
\begin{align*}
A'_{\mu\mu\rho\ldots} &=
	b_{\mu\alpha} b_{\mu\beta} b_{\varrho\gamma} \ldots
	A_{\alpha\beta\gamma\ldots}
=
	\delta_{\alpha\beta} b_{\rho\gamma} \ldots
	A_{\alpha\beta\gamma\ldots}
\\
&=
b_{\rho\gamma} \ldots A_{\alpha\alpha\gamma\ldots}
\end{align*}

Zu diesen elementaren Rechnungsregeln tritt noch die der Tensorbildung 
(Erweiterung) durch Differentiation
\begin{align}
T_{\mu\nu\varrho\ldots\alpha} = \frpa{A_{\mu\nu\varrho}}{x_{\alpha}}.
\label{eq:12}
\end{align}

Wenn $(A)$ ein Tensor vom Range $\alpha$ ist, so ist $(T)$ ein Tensor vom Range 
$\alpha + 1$. Der Beweis folgt aus den Transformationsgleichungen \eqref{eq:3a} 
und \eqref{eq:5}, aus welch letzteren man schließt:
\begin{align}
\frpa{}{x'_{\nu}} = 
\frpa{}{x_{\alpha}} \cdot \frpa{x_{\alpha}}{x'_{\nu}} = 
b_{\nu\alpha} \frpa{}{x_{\alpha}}
\end{align}

Gemäß diesen Rechnungsregeln lassen sich aus Tensoren (bezüglich linearer 
orthogonaler Transformationen) neue ableiten.

Symmetrieeigenschaften der Tensoren. Tensoren heißen symmetrisch bzw.\ 
antisymmetrisch bezüglich zweier ihrer Indizes $\mu$ und $\nu$, wenn die beiden 
Komponenten, die aus der Vertauschung der Indizes $\mu$ und $\nu$ auseinander 
hervorgehen, einander gleich bzw.\ entgegengesetzt gleich sind.
\begin{align*}
&\text{Bedingung der Symmetrie:} && A_{\mu\nu\varrho} = A_{\nu\mu\varrho}
\\
&\text{Bedingung der Antisymmetrie:} && A_{\mu\nu\varrho} = -A_{\nu\mu\varrho}
\\
\end{align*}

Satz: Der Charakter der Symmetrie bzw.\ Antisymmetrie besteht unabhängig von 
der Koordinatenwahl, durch welchen Satz er erst wirklich Bedeutung erhält. 
Beweis aus der Definitionsgleichung der Tensoren.

\emss{Spezielle Tensoren.}
\begin{enumerate}[I.]
\item
Die Größen $\delta_{\varrho\sigma}$ [\cref{eq:4}] sind Tensorkomponenten 
(Fundamentaltensor).

\item
Es gibt einen bezüglich aller Indexpaare antisymmetrischen Tensor
\end{enumerate}

Diese wenigen einfachen Sätze bilden -- wie sich im folgenden zeigen wird -- 
den invariantentheoretischen Apparat für den Aufbau der Gleichungen der 
vorrelativistischen Physik und der speziellen Relativitätstheorie.


\begin{align}
&\left.
\begin{aligned}
&\frpa{\mfrakh_3}{x_2} - \frpa{\mfrakh_2}{x_3} =
\frac{1}{c} \frpa{\mfrake_1}{t} + \frac{1}{c} \mfraki_1
\\
&\frpa{\mfrakh_1}{x_3} - \frpa{\mfrakh_3}{x_1} =
\frac{1}{c} \frpa{\mfrake_2}{t} + \frac{1}{c} \mfraki_2
\\
&\ldots
\\
&\frpa{\mfrake_1}{x_1} + \frpa{\mfrake_2}{x_2} + \frpa{\mfrake_3}{x_3} = \varrho
\end{aligned}
\right\}
\label{eq:19} \\
&\left.
\begin{aligned}
&\frpa{\mfrake_3}{x_2} - \frpa{\mfrake_2}{x_3} =
-\frac{1}{c} \frpa{\mfrakh_1}{t}
\\
&\frpa{\mfrake_1}{x_3} - \frpa{\mfrake_3}{x_1} =
-\frac{1}{c} \frpa{\mfrakh_2}{t}
\\
&\ldots
\\
&\frpa{\mfrakh_1}{x_1} + \frpa{\mfrakh_2}{x_2} + \frpa{\mfrakh_3}{x_3} = 0
\end{aligned}
\right\}
\label{eq:20}
\end{align}


$\mfraki$ ist ein Vektor, da die Stromdichte definiert ist als 
Elektrizitätsdichte, multipliziert mit dem Geschwindigkeitsvektor der 
Elektrizität. Also ist es nach den ersten drei Gleichungen naheliegend, 
auch $\mfrake$ als einen Vektor zu betrachten. Dann können wir $\mfrakh$ nicht 
als Vektor auffassen\footnote{Diese Betrachtungen sollen den Leser mit der 
Tensorbetrachtung bekannt machen ohne die besonderen Schwierigkeiten der 
vierdimensionalen Betrachtungsweise, damit dann die entsprechenden 
Betrachtungen der speziellen Relativitätstheorie (\textsc{Minkowski}s 
Interpretation des Feldes) weniger Schwierigkeiten machen.}. Die Gleichungen 
lassen sich aber leicht interpretieren, indem man $\mfrakh$ als 
antisymmetrischen Tensor vom Range $2$ interpretiert. Wir schreiben in diesem 
Sinne statt $\mfrakh_{1}$, $\mfrakh_{2}$, $\mfrakh_{3}$ der Reihe nach 
$\mfrakh_{23}$, $\mfrakh_{31}$, $\mfrakh_{12}$. Mit Rücksicht auf die 
Antisymmetrie von $\mfrakh_{\mu\nu}$ können die ersten drei Gleichungen von 
\eqref{eq:19} und \eqref{eq:20} in die Form gebracht werden
\begin{align}
	&= \frac{1}{c}
	\tag{19a} \\
	&= +\frac{1}{c}.
	\tag{20a}
\end{align}
$\mfrakh$ erscheint demnach im Gegensatz zu $\mfrake$ als Größe vom 
Symmetriecharakter eines Drehmomentes oder einer Rotationsgeschwindigkeit. Die 
Divergenzgleichungen aber nehmen die Formen an
\begin{align}
	\frpa{\mfrake_{\nu}}{x_{\nu}}&= \varrho
	\tag{19b} \\
	\frpa{\mfrakh_{\mu\nu}}{x_{\varrho}} +
	\frpa{\mfrakh_{\nu\varrho}}{x_{\mu}} +
	\frpa{\mfrakh_{\varrho\mu}}{x_{\nu}} &= 0\,.
	\tag{20b}
\end{align}
Die letzte Gleichung ist eine antisymmetrische Tensorgleichung vom dritten Range 
(die Antisymmetrie der linken Seite bezüglich jedes Indexpaares ist mit 
Rücksicht auf die Antisymmetrie von $\mfrakh_{\mu\nu}$ leicht zu beweisen).

Sie enthält also trotz ihrer drei Indizes nur eine einzige Bedingung. Diese 
Schreibweise ist darum natürlicher als die übliche, weil sie im Gegensatz zu 
letzterer ohne Zeichenänderung auf kartesische Linkssysteme wie auf 
Rechtssysteme paßt.